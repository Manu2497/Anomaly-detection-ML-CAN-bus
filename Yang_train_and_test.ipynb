{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"kqD6rF1FuccX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667035610523,"user_tz":-120,"elapsed":17733,"user":{"displayName":"EMANUELE BIANCHI","userId":"05302933043291015822"}},"outputId":"135a6f09-b93c-4620-dc41-f020acd8fdbc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import sklearn as sk\n","import pandas as pd\n","import os \n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","from xgboost import XGBClassifier\n","from sklearn.model_selection import cross_val_score\n","from imblearn.over_sampling import SMOTE\n","from sklearn.metrics import f1_score\n","from imblearn.over_sampling import SMOTE\n","from sklearn.ensemble import StackingClassifier\n","\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","def read_CAN_trace(file):\n","    dataset = pd.read_csv(file, sep=',')\n","    return dataset\n","\n","def load_dataset(folder):\n","    datasets = []\n","    for filename in os.listdir(folder):\n","        f = os.path.join(folder, filename)\n","        datasets.append(read_CAN_trace(f))\n","\n","    dataset = pd.DataFrame(columns=datasets[0].columns)\n","    for d in datasets:\n","        dataset=pd.concat([dataset, d], ignore_index=True)\n","    return dataset\n","\n","def from_hex_to_float(val):\n","  val = float.fromhex(val)\n","  return val\n","\n","def normalize_dataset(dataset):\n","  MAX_VALUE_CAN_ID = max(dataset['CAN_ID'])\n","  MIN_VALUE_CAN_ID = min(dataset['CAN_ID'])\n","  DIFF_MAX_MIN_ID = MAX_VALUE_CAN_ID - MIN_VALUE_CAN_ID\n","\n","  def normalize_value_ID(val):\n","    x_n = (val - MIN_VALUE_CAN_ID)/DIFF_MAX_MIN_ID\n","    return x_n\n","  dataset['CAN_ID'] = dataset['CAN_ID'].apply(normalize_value_ID)\n","  \n","  return dataset\n","\n","def dataset_preprocessing(dataset):\n","  from numpy import float64, float32\n","  dataset=dataset.dropna() \n","  dataset['CAN_ID'] =dataset['CAN_ID'].apply(from_hex_to_float)\n","  dataset['PAYLOAD_HEX']=dataset['PAYLOAD_HEX'].apply(from_hex_to_float)\n","  dataset['ANOMALY']=dataset['ANOMALY'].astype(bool)\n","  dataset['PAYLOAD_BIN']=dataset['PAYLOAD_BIN'].astype(float32)\n","  dataset['PAYLOAD_HEX']=dataset['PAYLOAD_HEX'].astype(float32)\n","  dataset=dataset.drop(columns=['PAYLOAD_BIN', 'DLC', 'timestamp']) \n","\n","  dataset = normalize_dataset(dataset)\n","  return dataset\n","\n","def print_results(y_true, predicted):\n","  print(f'accuracy  --> {accuracy_score(y_true,predicted)}')\n","  print(f'Confusion Matrix --> \\n {confusion_matrix(y_true,predicted)}')\n","  print(f'F-1 Score --> {f1_score(y_true, predicted)}')\n","\n","\n","def make_dataset_id(paths):\n","  #Make Pandas Dataframe from multiple paths of can txt trace\n","  datasets = []\n","  for p in paths:\n","    datasets.append(read_CAN_trace(p))\n","\n","  dataset = pd.DataFrame(columns=datasets[0].columns)\n","  for d in datasets:\n","    dataset=pd.concat([dataset, d], ignore_index=True)\n","  return dataset\n","\n","\n","def make_train_and_test(paths):\n","  dataset = make_dataset_id(paths)\n","  dataset = dataset_preprocessing(dataset)\n","\n","  x_train, x_test, y_train, y_test = train_test_split(dataset.drop(columns=['ANOMALY']), dataset['ANOMALY'], random_state=0, train_size=(3/4), shuffle=True)\n","  \n","  smote = SMOTE(random_state = 2)\n","  x_train_res, y_train_res = smote.fit_resample(x_train, y_train)\n","\n","  estimators = [\n","              ('dt', DecisionTreeClassifier(max_depth=8, min_samples_split=8, min_samples_leaf=3)),\n","              ('rf', RandomForestClassifier(verbose = 2, n_estimators = 200, max_depth=8, min_samples_split=8, min_samples_leaf=3, n_jobs=-1)),\n","              ('ext', ExtraTreesClassifier(verbose = 2, n_estimators = 200, max_depth=8, min_samples_split=8, min_samples_leaf=3, n_jobs=-1)),\n","              ('xgbc', XGBClassifier(n_estimators = 200, tree_method = 'exact'))\n","\n","  ]\n","\n","  stackClassifier = StackingClassifier(estimators = estimators, verbose = 3)\n","  stackClassifier.fit(x_train_res, y_train_res)\n","\n","  pred = stackClassifier.predict(x_test)\n","  print_results(y_test, pred)\n","\n","  return stackClassifier\n","\n","def make_SMOTE(x_train, y_train):\n","  smote = SMOTE(random_state = 2)\n","  x_train_res, y_train_res = smote.fit_resample(x_train, y_train)\n","\n","  return x_train_res, y_train_res\n","\n","def save_pickle_file(file_name, var_to_dump ):\n","  with open(file_name, \"wb\") as open_file:\n","    pickle.dump(var_to_dump, open_file)\n","def load_pickle_file(file_name):\n","  with open(file_name, \"rb\") as open_file:\n","    f = pickle.load(open_file)\n","  return f"]},{"cell_type":"code","source":["\"\"\"\n","n_7 OSR\n","\"\"\"\n","\n","path_file_1 = \"/Dataset_DAGA/infected/OrderedSequenceReplay/n_7_V40_01.can.txt\"\n","path_file_2 = \"/Dataset_DAGA/infected/OrderedSequenceReplay/n_7_V40_02.can.txt\"\n","path_file_3 = \"/Dataset_DAGA/infected/OrderedSequenceReplay/n_7_V40_03.can.txt\"\n","path_file_4 = \"/Dataset_DAGA/infected/OrderedSequenceReplay/n_7_V40_04.can.txt\"\n","path_file_5 = \"/Dataset_DAGA/infected/OrderedSequenceReplay/n_7_V40_05.can.txt\"\n","path_file_6 = \"/Dataset_DAGA/infected/OrderedSequenceReplay/n_7_V40_06.can.txt\"\n","path_file_7 = \"/Dataset_DAGA/infected/OrderedSequenceReplay/n_7_V40_07.can.txt\"\n","\n","paths = [path_file_1, path_file_2, path_file_3, path_file_4]"],"metadata":{"id":"ZWfw_nnQMJKk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = make_dataset_id(paths)\n","dataset = dataset_preprocessing(dataset)\n","dataset"],"metadata":{"id":"XL7FMZYph4bC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classifier = make_train_and_test(paths)\n","\"\"\"\n","Saving models\n","\"\"\"\n","import pickle\n","\n","file_name = \"/DAGA_classifier/OSR_n_7_StackClassifier.pkl\"\n","with open(file_name, \"wb\") as open_file:\n","  pickle.dump(classifier, open_file)"],"metadata":{"id":"btL13swKMaD8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = read_CAN_trace(path_file_5)\n","dataset = dataset_preprocessing(dataset)\n","\n","Y = dataset['ANOMALY']\n","x = dataset.drop(columns=['ANOMALY'])\n","\n","pred = classifier.predict(x)\n","print(\"results: .......\")\n","print_results(Y, pred)"],"metadata":{"id":"G7oasIdTiBNj"},"execution_count":null,"outputs":[]}]}